{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(device)  # Move the model to the GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model in the DataParallel wrapper\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to remove numbers and double \"##\", remove punctuation, lowercase all text, remove stop words, stem words, and lemmatize words\n",
    "def remove_numbers_and_double_hash(text):\n",
    "  text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "  text = re.sub(r\"##\", \"\", text)\n",
    "  return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "  return re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "def lowercase_text(text):\n",
    "  return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "  return [word for word in text if word not in stop_words]\n",
    "\n",
    "def stem_words(text):\n",
    "  stemmer = nltk.PorterStemmer()\n",
    "  return [stemmer.stem(word) for word in text]\n",
    "\n",
    "def lemmatize_words(text):\n",
    "  lemmatizer = nltk.WordNetLemmatizer()\n",
    "  return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "# Functions to preprocess text and get BERT embeddings\n",
    "def preprocess_text(text):\n",
    "  # Add more preprocessing steps as needed\n",
    "  text = remove_numbers_and_double_hash(text)\n",
    "  text = remove_punctuation(text)\n",
    "  text = lowercase_text(text)\n",
    "  text = remove_stop_words(text)\n",
    "  text = stem_words(text)\n",
    "  text = lemmatize_words(text)\n",
    "  return text\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "  # This function should be defined with the appropriate model and tokenizer setup.\n",
    "  # Make sure to define and load the model and tokenizer before calling this function.\n",
    "  inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "  embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "  return embeddings.cpu().numpy()\n",
    "\n",
    "# Functions to handle both pre-processing and BERT embeddings\n",
    "def preprocess_and_get_embeddings(row):\n",
    "  # Get text from the row\n",
    "  text = row['Text']\n",
    "\n",
    "  # Preprocess text\n",
    "  preprocessed_text = preprocess_text(text)\n",
    "\n",
    "  # Get BERT embeddings\n",
    "  embeddings = get_bert_embeddings(preprocessed_text)\n",
    "\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display BERT embeddings and preprocessed text\n",
    "def display_bert_embedding_and_preprocessing(data, row_idx):\n",
    "  # Get BERT embedding for the specified row\n",
    "  embedding = data.iloc[row_idx, -768:]\n",
    "\n",
    "  # Get preprocessed text for the specified row\n",
    "  preprocessed_text = data.iloc[row_idx]['Text']\n",
    "\n",
    "  # Display BERT embedding\n",
    "  print(\"BERT Embeddings:\")\n",
    "  print(embedding)\n",
    "\n",
    "  # Display preprocessed text\n",
    "  print(\"\\nPreprocessed Text:\")\n",
    "  print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv('DataFinal_1.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ï»¿Title', 'Abstract'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the list of column names\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 'Title' and 'Abstract' columns into a single column called 'Text'\n",
    "data['Text'] = data['ï»¿Title'].fillna('') + ' ' + data['Abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Process and obtain embeddings for each row\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeddings \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mapply(preprocess_and_get_embeddings, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(embeddings\u001b[39m.\u001b[39mto_numpy())  \u001b[39m# Convert to a NumPy array\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9412\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9414\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9415\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9416\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9421\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9422\u001b[0m )\n\u001b[1;32m-> 9423\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    676\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 798\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    800\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    812\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    813\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    815\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    816\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    817\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m text \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Preprocess text\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m preprocessed_text \u001b[39m=\u001b[39m preprocess_text(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Get BERT embeddings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m embeddings \u001b[39m=\u001b[39m get_bert_embeddings(preprocessed_text)\n",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   \u001b[39m# Add more preprocessing steps as needed\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   text \u001b[39m=\u001b[39m remove_numbers_and_double_hash(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   text \u001b[39m=\u001b[39m remove_punctuation(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   text \u001b[39m=\u001b[39m lowercase_text(text)\n",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_numbers_and_double_hash\u001b[39m(text):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m[0-9]+\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m##\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\re.py:209\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    203\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Process and obtain embeddings for each row\n",
    "embeddings = data.apply(preprocess_and_get_embeddings, axis=1)\n",
    "embeddings = np.vstack(embeddings.to_numpy())  # Convert to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the embeddings with the original DataFrame\n",
    "data = pd.concat([data, pd.DataFrame(embeddings)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251014, 768)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of text_embeddings\n",
    "#text_embeddings = embeddings  # Assign the embeddings to text_embeddings\n",
    "print(embeddings.shape)  # Should be (number_of_samples, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Choose a row to display (change row_idx to the desired row)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m row_idx \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m \u001b[39m# Change this to the index of the row you want to display\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m display_bert_embedding_and_preprocessing(data, row_idx)\n",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m embedding \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39miloc[row_idx, \u001b[39m-\u001b[39m\u001b[39m768\u001b[39m:]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Get preprocessed text for the specified row\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m preprocessed_text \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49miloc[row_idx][\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Display BERT embedding\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBERT Embeddings:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   1009\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# Choose a row to display (change row_idx to the desired row)\n",
    "row_idx = 200 # Change this to the index of the row you want to display\n",
    "display_bert_embedding_and_preprocessing(data, row_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "data.to_csv('sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import fcluster, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cluster_analysis(embeddings, linkage_matrix, min_clusters, max_clusters, word_list):\n",
    "\n",
    "    results = pd.DataFrame(columns=['Num_Clusters', 'Silhouette_Score', 'Cohesion_Score', 'Separation_Score', 'Cluster_Assignments'])\n",
    "\n",
    "    for num_clusters in range(min_clusters, max_clusters + 1):\n",
    "        # Perform hierarchical clustering and assign clusters\n",
    "        cluster_assignments = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(embeddings, cluster_assignments)\n",
    "\n",
    "        # Calculate cohesion and separation scores\n",
    "        cluster_centers = []\n",
    "        max_cluster_size = 0\n",
    "\n",
    "        for cluster_id in range(1, num_clusters + 1):\n",
    "            cluster_indices = np.where(cluster_assignments == cluster_id)[0]\n",
    "            cluster_words = [word_list[i] for i in cluster_indices]\n",
    "            cluster_centers.append(cluster_words)\n",
    "\n",
    "            # Track the maximum cluster size\n",
    "            max_cluster_size = max(max_cluster_size, len(cluster_indices))\n",
    "\n",
    "        # Pad shorter clusters with a placeholder value (e.g., '')\n",
    "        cluster_centers = [cluster + [''] * (max_cluster_size - len(cluster)) for cluster in cluster_centers]\n",
    "\n",
    "        # Convert the cluster_centers list to a two-dimensional NumPy array\n",
    "        cluster_centers = np.array(cluster_centers, dtype='object')\n",
    "\n",
    "        # Replace '' with a placeholder numeric value (e.g., 0) for numerical calculations\n",
    "        cluster_centers[cluster_centers == ''] = np.NAN\n",
    "\n",
    "        # Convert the dtype to a numeric type\n",
    "        cluster_centers = cluster_centers.astype(float, casting='unsafe')\n",
    "\n",
    "        # Calculate pairwise distances using squareform to handle the new numeric placeholder\n",
    "        pairwise_distances = pdist(cluster_centers, metric='euclidean')\n",
    "        pairwise_distances = squareform(pairwise_distances)\n",
    "\n",
    "        cohesion_score = np.mean(pairwise_distances)\n",
    "        separation_score = np.min(pairwise_distances)\n",
    "\n",
    "        # Create a new row for the results DataFrame\n",
    "        new_row = {'Num_Clusters': num_clusters, 'Silhouette_Score': silhouette_avg, 'Cohesion_Score': cohesion_score, 'Separation_Score': separation_score, 'Cluster_Assignments': cluster_assignments}\n",
    "\n",
    "        # Append the new row to the results DataFrame\n",
    "        results = results.append(new_row, ignore_index=True)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'driver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m max_clusters \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Assume embeddings and linkage_matrix are defined\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m results \u001b[39m=\u001b[39m perform_cluster_analysis(embeddings, linkage_matrix, min_clusters, max_clusters, word_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Print the cluster analysis results\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(results\u001b[39m.\u001b[39mto_string())\n",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m cluster_centers[cluster_centers \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mNAN\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Convert the dtype to a numeric type\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m cluster_centers \u001b[39m=\u001b[39m cluster_centers\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m, casting\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39munsafe\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Calculate pairwise distances using squareform to handle the new numeric placeholder\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m pairwise_distances \u001b[39m=\u001b[39m pdist(cluster_centers, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'driver'"
     ]
    }
   ],
   "source": [
    "# Tokenize the text in your dataset\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data['Tokens'] = data['Text'].apply(lambda x: [token for token in tokenizer.tokenize(x) if token not in stop_words])\n",
    "\n",
    "# Create a list of words based on your tokens\n",
    "word_list = [word for tokens in data['Tokens'] for word in tokens]\n",
    "\n",
    "# Calculate the pairwise distances between the data points\n",
    "pairwise_distances = pdist(embeddings, metric='euclidean')\n",
    "\n",
    "# Perform hierarchical clustering using DIANA\n",
    "linkage_matrix = linkage(pairwise_distances, method='ward', metric='euclidean')\n",
    "\n",
    "# Example usage\n",
    "min_clusters = 2\n",
    "max_clusters = 10\n",
    "\n",
    "# Assume embeddings and linkage_matrix are defined\n",
    "results = perform_cluster_analysis(embeddings, linkage_matrix, min_clusters, max_clusters, word_list)\n",
    "\n",
    "# Print the cluster analysis results\n",
    "print(results.to_string())\n",
    "\n",
    "# Choose the optimal cluster assignments\n",
    "cluster_assignments = results.loc[(results['Cohesion_Score'] > 0.1) & (results['Separation_Score'] > 0.2), 'Cluster_Assignments'].values[0]\n",
    "\n",
    "# Add cluster assignments to the DataFrame\n",
    "data['Cluster'] = cluster_assignments\n",
    "\n",
    "# Print the cluster analysis results\n",
    "print(results.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
