{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(device)  # Move the model to the GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model in the DataParallel wrapper\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_without_punct = text.translate(translator)\n",
    "    return re.sub(r'\\s+', ' ', text_without_punct).strip()  # Replace consecutive spaces with a single space\n",
    "\n",
    "\n",
    "# Function to lowercase text\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatized_words = []\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in stop_words:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            lemmatized_words.append(word)  # Keep stopwords unchanged\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Add more preprocessing steps as needed\n",
    "    text = remove_punctuation(text)\n",
    "    text = lowercase_text(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    # This function should be defined with the appropriate model and tokenizer setup.\n",
    "    # Make sure to define and load the model and tokenizer before calling this function.\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display BERT embeddings and preprocessed text\n",
    "def display_bert_embedding_and_preprocessing(data, row_idx):\n",
    "    # Get BERT embeddings for the specified row\n",
    "    embedding = data.iloc[row_idx, -768:]  # Assuming the embeddings have 768 dimensions\n",
    "    preprocessed_text = data.iloc[row_idx]['Text']\n",
    "\n",
    "    # Display BERT embeddings\n",
    "    print(\"BERT Embeddings:\")\n",
    "    print(embedding)\n",
    "    \n",
    "    # Display preprocessed text\n",
    "    print(\"\\nPreprocessed Text:\")\n",
    "    print(preprocessed_text)\n",
    "    \n",
    "def preprocess_and_get_embeddings(row):\n",
    "    text = row.get('Text', '')  # Use get() to handle missing values\n",
    "    embeddings = get_bert_embeddings(text)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ï»¿Title', 'Abstract'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv('DataFinal_1.csv', encoding='latin-1')\n",
    "\n",
    "# Print the list of column names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 'Title' and 'Abstract' columns into a single column called 'Text'\n",
    "data['Text'] = data['ï»¿Title']+ ',' + data['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame\n",
    "data['Text'] = data['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and obtain embeddings for each row\n",
    "embeddings = data.apply(preprocess_and_get_embeddings, axis=1)\n",
    "embeddings = np.vstack(embeddings.to_numpy())  # Convert to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the embeddings with the original DataFrame\n",
    "data = pd.concat([data, pd.DataFrame(embeddings)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326, 768)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of text_embeddings\n",
    "#text_embeddings = embeddings  # Assign the embeddings to text_embeddings\n",
    "print(embeddings.shape)  # Should be (number_of_samples, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Embeddings:\n",
      "0     -0.208345\n",
      "1      0.235173\n",
      "2      0.290928\n",
      "3      0.064327\n",
      "4      0.105875\n",
      "         ...   \n",
      "763     -0.0644\n",
      "764   -0.435931\n",
      "765   -0.227162\n",
      "766    0.120141\n",
      "767    0.546278\n",
      "Name: 200, Length: 768, dtype: object\n",
      "\n",
      "Preprocessed Text:\n",
      "medical supply inventory distribution system pnp hospitalthe pharmacy pnp regional xiii health service hospital us traditional way inventory medical supply equipment performing daily transaction paper pen used recording supply thus result poor inventory management product availability monitoring deemed necessary response pnp working environment medical supply inventory system pnp hospital designed help pnp hospital improve staff work efficiency computerizing part business process automation inventory monitoring medicine product done properly cope high demand pnp keywords information system inventory inventory system laravel postgresql\n"
     ]
    }
   ],
   "source": [
    "# Choose a row to display (change row_idx to the desired row)\n",
    "row_idx = 200 # Change this to the index of the row you want to display\n",
    "display_bert_embedding_and_preprocessing(data, row_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fcluster' from 'sklearn.cluster' (C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\XuneCode\\Train2.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train2.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train2.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m silhouette_score\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/XuneCode/Train2.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcluster\u001b[39;00m \u001b[39mimport\u001b[39;00m fcluster\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'fcluster' from 'sklearn.cluster' (C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the distance matrix between all pairs of data points.\n",
    "def compute_distance_matrix(data):\n",
    "    distance_matrix = np.zeros((data.shape[0], data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(i + 1, data.shape[0]):\n",
    "            distance_matrix[i][j] = np.linalg.norm(data[i] - data[j])\n",
    "    return distance_matrix\n",
    "\n",
    "# Performs DIANA hierarchical clustering.\n",
    "def perform_diana_clustering(distance_matrix):\n",
    "    linkage_matrix = linkage(distance_matrix, method=\"ward\")\n",
    "    cluster_assignments = np.array(dendrogram(linkage_matrix)[\"leaves\"])\n",
    "    return cluster_assignments\n",
    "\n",
    "def calculate_silhoutte_score(embeddings, cluster_assignments):\n",
    "    silhouette_score = silhouette_score(embeddings, cluster_assignments)\n",
    "    return silhouette_score\n",
    "\n",
    "def calculate_cohesion(embeddings, cluster_assignments):\n",
    "    cohesion_score = 0\n",
    "    for cluster_id in range(1, np.max(cluster_assignments) + 1):\n",
    "      cluster_indices = np.where(cluster_assignments == cluster_id)[0]\n",
    "      cluster_embeddings = embeddings[cluster_indices]\n",
    "\n",
    "      cluster_centroid = np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "      cohesion_score += np.sum(np.linalg.norm(cluster_embeddings - cluster_centroid))\n",
    "\n",
    "    return cohesion_score\n",
    "\n",
    "def calculate_separation(embeddings, cluster_assignments):\n",
    "    separation_score = 0\n",
    "    for cluster_id in range(1, np.max(cluster_assignments) + 1):\n",
    "      cluster_indices = np.where(cluster_assignments == cluster_id)[0]\n",
    "      cluster_embeddings = embeddings[cluster_indices]\n",
    "\n",
    "      cluster_centroid = np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "      other_cluster_indices = np.where(cluster_assignments != cluster_id)[0]\n",
    "      other_cluster_embeddings = embeddings[other_cluster_indices]\n",
    "\n",
    "      separation_score += np.sum(np.linalg.norm(cluster_embeddings - other_cluster_embeddings))\n",
    "\n",
    "    return separation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to a NumPy array.\n",
    "embeddings = data.to_numpy()\n",
    "\n",
    "# Perform DIANA hierarchical clustering.\n",
    "cluster_assignments = perform_diana_clustering(embeddings)\n",
    "\n",
    "# Calculate the silhouette score, cohesion, and separation\n",
    "silhouette_score = calculate_silhoutte_score(embeddings, cluster_assignments)\n",
    "cohesion_score = calculate_cohesion(embeddings, cluster_assignments)\n",
    "separation_score = calculate_separation(embeddings, cluster_assignments)\n",
    "\n",
    "# Print the results\n",
    "print(\"Silhouette Score:\", silhouette_score)\n",
    "print(\"Cohesion Score:\", cohesion_score)\n",
    "print(\"Separation Score:\", separation_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "  # Load the data from the CSV file.\n",
    "  data = load_data(\"sample_data-1.csv\")\n",
    "\n",
    "  # Compute the distance matrix.\n",
    "  distance_matrix = compute_distance_matrix(data)\n",
    "\n",
    "  # Perform DIANA hierarchical clustering.\n",
    "  cluster_assignments = perform_diana_clustering(distance_matrix)\n",
    "\n",
    "  # Calculate the silhouette score, cohesion, and separation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
