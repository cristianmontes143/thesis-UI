{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_without_punct = text.translate(translator)\n",
    "    return text_without_punct\n",
    "\n",
    "# Function to lowercase text\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenize the text and convert it to input IDs\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Extract the embeddings for the [CLS] token (you can also use [CLS], [SEP], or average over all tokens)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv('FinalDataF.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Title_Embeddings</th>\n",
       "      <th>Abstract_Embeddings</th>\n",
       "      <th>Title_Cluster</th>\n",
       "      <th>Abstract_Cluster</th>\n",
       "      <th>Title_Cluster_Labels</th>\n",
       "      <th>Abstract_Cluster_Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A CASE STUDY OF DRIVER'S LICENSE PROCESSES ON ...</td>\n",
       "      <td>A Case Study on Driver's License Processes was...</td>\n",
       "      <td>[[-0.05910889804363251, 0.26362770795822144, -...</td>\n",
       "      <td>[[-0.296672523021698, 0.3988223075866699, 0.31...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Cluster 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A CASE STUDY ON POULTRY EGG PRODUCTION BUSINESS</td>\n",
       "      <td>A Case Study on Poultry Egg Production Busines...</td>\n",
       "      <td>[[-0.2676674425601959, 0.0976429432630539, -0....</td>\n",
       "      <td>[[-0.22526228427886963, 0.14748013019561768, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Cluster 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESIGN AND DEVELOPMENT OF A FIRE DETECTION AND...</td>\n",
       "      <td>A fire detection and alarm system prototype ba...</td>\n",
       "      <td>[[-0.41528111696243286, -0.09206049889326096, ...</td>\n",
       "      <td>[[-0.302248477935791, 0.34940415620803833, 0.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Cluster 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A KNOWLEDGE-BASED SYSTEM N MATCHING TREE, PLAN...</td>\n",
       "      <td>A Knowledge-Based System on Matching of Tree, ...</td>\n",
       "      <td>[[-0.7902029752731323, 0.06406101584434509, -0...</td>\n",
       "      <td>[[-0.29129481315612793, 0.33515429496765137, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Cluster 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DESIGN AND DEVELOPMENT OF MEMORANDUM PRIORITIZ...</td>\n",
       "      <td>A memorandum is a means of inter-office corres...</td>\n",
       "      <td>[[-0.7163332104682922, 0.11507435888051987, -0...</td>\n",
       "      <td>[[-0.695845901966095, 0.15343907475471497, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cluster 0</td>\n",
       "      <td>Cluster 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  A CASE STUDY OF DRIVER'S LICENSE PROCESSES ON ...   \n",
       "1    A CASE STUDY ON POULTRY EGG PRODUCTION BUSINESS   \n",
       "2  DESIGN AND DEVELOPMENT OF A FIRE DETECTION AND...   \n",
       "3  A KNOWLEDGE-BASED SYSTEM N MATCHING TREE, PLAN...   \n",
       "4  DESIGN AND DEVELOPMENT OF MEMORANDUM PRIORITIZ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  A Case Study on Driver's License Processes was...   \n",
       "1  A Case Study on Poultry Egg Production Busines...   \n",
       "2  A fire detection and alarm system prototype ba...   \n",
       "3  A Knowledge-Based System on Matching of Tree, ...   \n",
       "4  A memorandum is a means of inter-office corres...   \n",
       "\n",
       "                                    Title_Embeddings  \\\n",
       "0  [[-0.05910889804363251, 0.26362770795822144, -...   \n",
       "1  [[-0.2676674425601959, 0.0976429432630539, -0....   \n",
       "2  [[-0.41528111696243286, -0.09206049889326096, ...   \n",
       "3  [[-0.7902029752731323, 0.06406101584434509, -0...   \n",
       "4  [[-0.7163332104682922, 0.11507435888051987, -0...   \n",
       "\n",
       "                                 Abstract_Embeddings  Title_Cluster  \\\n",
       "0  [[-0.296672523021698, 0.3988223075866699, 0.31...              0   \n",
       "1  [[-0.22526228427886963, 0.14748013019561768, 0...              0   \n",
       "2  [[-0.302248477935791, 0.34940415620803833, 0.2...              1   \n",
       "3  [[-0.29129481315612793, 0.33515429496765137, 0...              0   \n",
       "4  [[-0.695845901966095, 0.15343907475471497, 0.0...              0   \n",
       "\n",
       "   Abstract_Cluster Title_Cluster_Labels Abstract_Cluster_Labels  \n",
       "0                 0            Cluster 0               Cluster 0  \n",
       "1                 4            Cluster 0               Cluster 0  \n",
       "2                 2            Cluster 0               Cluster 0  \n",
       "3                 2            Cluster 0               Cluster 0  \n",
       "4                 0            Cluster 0               Cluster 0  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to preprocess the thesis titles and abstracts and get BERT embeddings\n",
    "def preprocess_and_get_embeddings(row):\n",
    "    title = row['Title']\n",
    "    abstract = row['Abstract']\n",
    "\n",
    "    # Check for NaN values and return them unchanged\n",
    "    if pd.isna(title):\n",
    "        title = \"\"\n",
    "    if pd.isna(abstract):\n",
    "        abstract = \"\"\n",
    "\n",
    "    title = remove_punctuation(title)\n",
    "    title = lowercase_text(title)\n",
    "    title = remove_stop_words(title)\n",
    "    title_embeddings = get_bert_embeddings(title)\n",
    "\n",
    "    abstract = remove_punctuation(abstract)\n",
    "    abstract = lowercase_text(abstract)\n",
    "    abstract = remove_stop_words(abstract)\n",
    "    abstract_embeddings = get_bert_embeddings(abstract)\n",
    "\n",
    "    return pd.Series({'Title_Embeddings': title_embeddings, 'Abstract_Embeddings': abstract_embeddings})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing and embedding function to each row of the DataFrame\n",
    "embeddings_data = data.apply(preprocess_and_get_embeddings, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the embeddings with the original DataFrame\n",
    "data = pd.concat([data, embeddings_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the title_embeddings to 2D\n",
    "title_embeddings = title_embeddings.reshape(title_embeddings.shape[0], -1)\n",
    "# Reshape the abstract_embeddings to 2D\n",
    "abstract_embeddings = abstract_embeddings.reshape(abstract_embeddings.shape[0], -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply divisive hierarchical clustering using AgglomerativeClustering for title embeddings\n",
    "n_clusters = 5  # Adjust the number of clusters as needed\n",
    "title_hierarchical_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "title_cluster_labels = title_hierarchical_clustering.fit_predict(title_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply divisive hierarchical clustering using AgglomerativeClustering for abstract embeddings\n",
    "abstract_hierarchical_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "abstract_cluster_labels = abstract_hierarchical_clustering.fit_predict(abstract_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to the DataFrame for title embeddings and abstract embeddings\n",
    "data['Title_Cluster'] = title_cluster_labels\n",
    "data['Abstract_Cluster'] = abstract_cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette score, cohesion, and separation for title embeddings\n",
    "silhouette_avg_title = silhouette_score(title_embeddings, title_cluster_labels)\n",
    "pairwise_dist_title = pairwise_distances(title_embeddings, metric='euclidean')\n",
    "cohesion_title = 0\n",
    "separation_title = 0\n",
    "for i in range(n_clusters):\n",
    "    cluster_i_indices = (title_cluster_labels == i)\n",
    "    cluster_i_embeddings = title_embeddings[cluster_i_indices]\n",
    "    cluster_i_dist = pairwise_dist_title[cluster_i_indices][:, cluster_i_indices]\n",
    "    cohesion_title += cluster_i_dist.sum() / 2.0  # Divide by 2 to avoid double counting\n",
    "    other_clusters_dist = pairwise_dist_title[cluster_i_indices][:, ~cluster_i_indices]\n",
    "    separation_title += other_clusters_dist.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize cohesion and separation by the number of samples for title embeddings\n",
    "num_samples_title = len(title_embeddings)\n",
    "cohesion_title /= num_samples_title\n",
    "separation_title /= num_samples_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score for Title Embeddings: 0.18864330649375916\n",
      "Cohesion for Title Embeddings: 330.10525384200247\n",
      "Separation for Title Embeddings: 3014.6395970394738\n",
      "Silhouette Score for Abstract Embeddings: 0.1572137176990509\n",
      "Cohesion for Abstract Embeddings: 249.62188977693256\n",
      "Separation for Abstract Embeddings: 2865.4959292763156\n"
     ]
    }
   ],
   "source": [
    "# Calculate silhouette score, cohesion, and separation for title embeddings\n",
    "silhouette_avg_title = silhouette_score(title_embeddings, title_cluster_labels)\n",
    "pairwise_dist_title = pairwise_distances(title_embeddings, metric='euclidean')\n",
    "cohesion_title = 0\n",
    "separation_title = 0\n",
    "for i in range(n_clusters):\n",
    "    cluster_i_indices = (title_cluster_labels == i)\n",
    "    cluster_i_embeddings = title_embeddings[cluster_i_indices]\n",
    "    cluster_i_dist = pairwise_dist_title[cluster_i_indices][:, cluster_i_indices]\n",
    "    cohesion_title += cluster_i_dist.sum() / 2.0  # Divide by 2 to avoid double counting\n",
    "    other_clusters_dist = pairwise_dist_title[cluster_i_indices][:, ~cluster_i_indices]\n",
    "    separation_title += other_clusters_dist.sum()\n",
    "\n",
    "# Normalize cohesion and separation by the number of samples for title embeddings\n",
    "num_samples_title = len(title_embeddings)\n",
    "cohesion_title /= num_samples_title\n",
    "separation_title /= num_samples_title\n",
    "\n",
    "# Calculate silhouette score, cohesion, and separation for abstract embeddings\n",
    "silhouette_avg_abstract = silhouette_score(abstract_embeddings, abstract_cluster_labels)\n",
    "pairwise_dist_abstract = pairwise_distances(abstract_embeddings, metric='euclidean')\n",
    "cohesion_abstract = 0\n",
    "separation_abstract = 0\n",
    "for i in range(n_clusters):\n",
    "    cluster_i_indices = (abstract_cluster_labels == i)\n",
    "    cluster_i_embeddings = abstract_embeddings[cluster_i_indices]\n",
    "    cluster_i_dist = pairwise_dist_abstract[cluster_i_indices][:, cluster_i_indices]\n",
    "    cohesion_abstract += cluster_i_dist.sum() / 2.0  # Divide by 2 to avoid double counting\n",
    "    other_clusters_dist = pairwise_dist_abstract[cluster_i_indices][:, ~cluster_i_indices]\n",
    "    separation_abstract += other_clusters_dist.sum()\n",
    "\n",
    "# Normalize cohesion and separation by the number of samples for abstract embeddings\n",
    "num_samples_abstract = len(abstract_embeddings)\n",
    "cohesion_abstract /= num_samples_abstract\n",
    "separation_abstract /= num_samples_abstract\n",
    "\n",
    "print(f\"Silhouette Score for Title Embeddings: {silhouette_avg_title}\")\n",
    "print(f\"Cohesion for Title Embeddings: {cohesion_title}\")\n",
    "print(f\"Separation for Title Embeddings: {separation_title}\")\n",
    "\n",
    "print(f\"Silhouette Score for Abstract Embeddings: {silhouette_avg_abstract}\")\n",
    "print(f\"Cohesion for Abstract Embeddings: {cohesion_abstract}\")\n",
    "print(f\"Separation for Abstract Embeddings: {separation_abstract}\")\n",
    "\n",
    "# Save the DataFrame with embeddings and clusters to a new CSV file\n",
    "data.to_csv('thesis_dataset_with_embeddings_and_clusters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame with embeddings and clusters\n",
    "data = pd.read_csv('thesis_dataset_with_embeddings_and_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cluster labels for both Title and Abstract embeddings\n",
    "title_cluster_labels = data['Title_Cluster']\n",
    "abstract_cluster_labels = data['Abstract_Cluster']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate cluster labels\n",
    "def generate_cluster_labels(cluster_labels):\n",
    "    cluster_counts = np.bincount(cluster_labels)\n",
    "    label_mapping = {cluster_id: f'Cluster {cluster_id}' for cluster_id in range(len(cluster_counts))}\n",
    "    cluster_labels = [label_mapping[cluster_id] for cluster_id in cluster_labels]\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels for Title Clusters\n",
    "title_cluster_labels = generate_cluster_labels(title_cluster_labels)\n",
    "\n",
    "# Generate labels for Abstract Clusters\n",
    "abstract_cluster_labels = generate_cluster_labels(abstract_cluster_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to the DataFrame\n",
    "data['Title_Cluster_Labels'] = title_cluster_labels\n",
    "data['Abstract_Cluster_Labels'] = abstract_cluster_labels\n",
    "\n",
    "# Save the DataFrame with cluster labels to a new CSV file\n",
    "data.to_csv('thesis_dataset_with_cluster_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
