{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string  # For remove_punctuation\n",
    "from nltk.corpus import stopwords  # For remove_stop_words\n",
    "from nltk.stem import PorterStemmer  # For stem_words\n",
    "from nltk.stem import WordNetLemmatizer  # For lemmatize_words\n",
    "import nltk\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 636kB/s]\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] \n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [00:17<00:00, 25.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_without_punct = text.translate(translator)\n",
    "    return text_without_punct\n",
    "\n",
    "# Function to lowercase text\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenize the text and convert it to input IDs\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Extract the embeddings for the [CLS] token (you can also use [CLS], [SEP], or average over all tokens)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv('FinalDataF.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_get_embeddings(row):\n",
    "    title = row['Title']\n",
    "    abstract = row['Abstract']\n",
    "\n",
    "    # Check for NaN values and return them unchanged\n",
    "    if pd.isna(title):\n",
    "        title = \"\"\n",
    "    if pd.isna(abstract):\n",
    "        abstract = \"\"\n",
    "\n",
    "    title = remove_punctuation(title)\n",
    "    title = lowercase_text(title)\n",
    "    title = remove_stop_words(title)\n",
    "    title_embeddings = get_bert_embeddings(title)\n",
    "\n",
    "    abstract = remove_punctuation(abstract)\n",
    "    abstract = lowercase_text(abstract)\n",
    "    abstract = remove_stop_words(abstract)\n",
    "    abstract_embeddings = get_bert_embeddings(abstract)\n",
    "\n",
    "    return pd.Series({'Title_Embeddings': title_embeddings, 'Abstract_Embeddings': abstract_embeddings})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\trainingthesis.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Apply the preprocessing and embedding function to each row of the DataFrame\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeddings_data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mapply(preprocess_and_get_embeddings, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Concatenate the embeddings with the original DataFrame\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([data, embeddings_data], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9412\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9414\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9415\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9416\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9421\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9422\u001b[0m )\n\u001b[1;32m-> 9423\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    676\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 798\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    800\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    812\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    813\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    815\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    816\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    817\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\trainingthesis.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m title \u001b[39m=\u001b[39m lowercase_text(title)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m title \u001b[39m=\u001b[39m remove_stop_words(title)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m title_embeddings \u001b[39m=\u001b[39m get_bert_embeddings(title)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m abstract \u001b[39m=\u001b[39m remove_punctuation(abstract)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m abstract \u001b[39m=\u001b[39m lowercase_text(abstract)\n",
      "\u001b[1;32mc:\\2nd-3rd-4th year\\4th year\\first semester\\thesis 2\\trainingthesis.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_bert_embeddings\u001b[39m(text):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Tokenize the text and convert it to input IDs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/2nd-3rd-4th%20year/4th%20year/first%20semester/thesis%202/trainingthesis.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# Extract the embeddings for the [CLS] token (you can also use [CLS], [SEP], or average over all tokens)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing and embedding function to each row of the DataFrame\n",
    "embeddings_data = data.apply(preprocess_and_get_embeddings, axis=1)\n",
    "\n",
    "# Concatenate the embeddings with the original DataFrame\n",
    "data = pd.concat([data, embeddings_data], axis=1)\n",
    "\n",
    "# Save the DataFrame with embeddings to a new CSV file\n",
    "data.to_csv('thesis_dataset_with_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
